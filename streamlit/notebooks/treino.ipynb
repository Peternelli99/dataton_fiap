{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Positives in train: 2374, Positives in val: 610\n",
      "[LightGBM] [Info] Number of positive: 2374, number of negative: 40633\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 431\n",
      "[LightGBM] [Info] Number of data points in the train set: 43007, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055200 -> initscore=-2.840004\n",
      "[LightGBM] [Info] Start training from score -2.840004\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.854524\tvalid_0's average_precision: 0.279775\tvalid_0's binary_logloss: 0.167005\n",
      "[100]\tvalid_0's auc: 0.854218\tvalid_0's average_precision: 0.284184\tvalid_0's binary_logloss: 0.166801\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's auc: 0.8558\tvalid_0's average_precision: 0.279808\tvalid_0's binary_logloss: 0.166567\n",
      "ROC AUC: 0.8558, Average Precision: 0.2798\n",
      "Fold 2 - Positives in train: 2396, Positives in val: 588\n",
      "[LightGBM] [Info] Number of positive: 2396, number of negative: 40611\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 428\n",
      "[LightGBM] [Info] Number of data points in the train set: 43007, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055712 -> initscore=-2.830238\n",
      "[LightGBM] [Info] Start training from score -2.830238\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.865725\tvalid_0's average_precision: 0.318774\tvalid_0's binary_logloss: 0.15709\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's auc: 0.862534\tvalid_0's average_precision: 0.321148\tvalid_0's binary_logloss: 0.160277\n",
      "ROC AUC: 0.8625, Average Precision: 0.3211\n",
      "Fold 3 - Positives in train: 2405, Positives in val: 579\n",
      "[LightGBM] [Info] Number of positive: 2405, number of negative: 40602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 43007, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055921 -> initscore=-2.826267\n",
      "[LightGBM] [Info] Start training from score -2.826267\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.869925\tvalid_0's average_precision: 0.330858\tvalid_0's binary_logloss: 0.154582\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's auc: 0.870701\tvalid_0's average_precision: 0.330977\tvalid_0's binary_logloss: 0.15546\n",
      "ROC AUC: 0.8707, Average Precision: 0.3310\n",
      "Fold 4 - Positives in train: 2359, Positives in val: 625\n",
      "[LightGBM] [Info] Number of positive: 2359, number of negative: 40648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 426\n",
      "[LightGBM] [Info] Number of data points in the train set: 43007, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.054852 -> initscore=-2.846712\n",
      "[LightGBM] [Info] Start training from score -2.846712\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.862976\tvalid_0's average_precision: 0.294647\tvalid_0's binary_logloss: 0.165669\n",
      "[100]\tvalid_0's auc: 0.862808\tvalid_0's average_precision: 0.297796\tvalid_0's binary_logloss: 0.165295\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's auc: 0.863847\tvalid_0's average_precision: 0.299506\tvalid_0's binary_logloss: 0.164972\n",
      "ROC AUC: 0.8638, Average Precision: 0.2995\n",
      "Fold 5 - Positives in train: 2402, Positives in val: 582\n",
      "[LightGBM] [Info] Number of positive: 2402, number of negative: 40606\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 43008, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055850 -> initscore=-2.827614\n",
      "[LightGBM] [Info] Start training from score -2.827614\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.87342\tvalid_0's average_precision: 0.334914\tvalid_0's binary_logloss: 0.154291\n",
      "[100]\tvalid_0's auc: 0.873928\tvalid_0's average_precision: 0.33229\tvalid_0's binary_logloss: 0.153549\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's auc: 0.874562\tvalid_0's average_precision: 0.334047\tvalid_0's binary_logloss: 0.153933\n",
      "ROC AUC: 0.8746, Average Precision: 0.3340\n",
      "Mean ROC AUC: 0.8655 ± 0.0066\n",
      "Mean Average Precision: 0.3131 ± 0.0206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/model_lgbm.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "df = pd.read_csv(\"../data/df_clean.csv\")\n",
    "X = df.drop(columns=[\"vaga_id\", \"codigo_candidato\", \"situacao_ord\"])\n",
    "y = (df[\"situacao_ord\"] == 5).astype(int)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "auc_scores = []\n",
    "pr_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=df[\"vaga_id\"])):\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Positives in train: {sum(y_train)}, Positives in val: {sum(y_val)}\")\n",
    "\n",
    "    # Pula fold se não houver positivos em treino ou validação\n",
    "    if sum(y_train) == 0 or sum(y_val) == 0:\n",
    "        print(f\"Skipping fold {fold + 1} due to lack of positive class\")\n",
    "        continue\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=[\"auc\", \"average_precision\"],\n",
    "        callbacks=[early_stopping(stopping_rounds=50), log_evaluation(period=50)],\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict_proba(X_val, num_iteration=model.best_iteration_)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    pr = average_precision_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"ROC AUC: {auc:.4f}, Average Precision: {pr:.4f}\")\n",
    "\n",
    "    auc_scores.append(auc)\n",
    "    pr_scores.append(pr)\n",
    "\n",
    "print(f\"Mean ROC AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"Mean Average Precision: {np.mean(pr_scores):.4f} ± {np.std(pr_scores):.4f}\")\n",
    "\n",
    "# Salve o modelo do último fold para uso posterior\n",
    "#model.booster_.save_model(\"../models/model_lgbm.txt\")\n",
    "\n",
    "\n",
    "joblib.dump(model, \"../models/model_lgbm.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanceamento das Classes\n",
    "Fold 1: 2.374 positivos (contratados) no treino, 610 na validação\n",
    "\n",
    "Fold 2: 2.396 positivos no treino, 588 na validação\n",
    "\n",
    "Taxa de positivos: ~5.5% (2.374/43.007), que é um balanceamento realista para recrutamento\n",
    "\n",
    "Performance do Modelo - Excelente\n",
    "ROC AUC: 0.8655 ± 0.0066 (86,55% de capacidade discriminativa)\n",
    "\n",
    "Average Precision: 0.3131 ± 0.0206 (31,31% de precisão média)\n",
    "\n",
    "Estabilidade: Baixo desvio padrão indica modelo consistente entre folds\n",
    "\n",
    "Interpretação das Métricas\n",
    "Métrica\tValor\tO que Significa\n",
    "ROC AUC = 0.87\tExcelente\tModelo distingue muito bem candidatos que serão contratados vs não contratados\n",
    "Average Precision = 0.31\tBom\tEm dados desbalanceados (5.5% positivos), conseguir 31% de precisão média é muito bom\n",
    "Early Stopping\t~68 iterações\tModelo convergiu sem overfitting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
